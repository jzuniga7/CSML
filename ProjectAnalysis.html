Practical Machine Learning Project
Jay Zuniga
Purpose

This is the course project for https://class.coursera.org/predmachlearn-034.
The goal of the project is to predict the manner in which participants performed a Unilateral Dumbbell Biceps curl in the Weight Lifting Exercise Data Set. Here the participants could perform the exercise in one of five ways:
Class A: According to specifications
Class B: Throwing elbows to the front
Class C: Lifting the dumbbell only halfway
Class D: Lowering the dumbbell only halfway
Class E: Throwing the hips to the front
Credits

This project is the dependent on the literature and data set provided by:
Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.
Read more: http://groupware.les.inf.puc-rio.br/har#ixzz3s95vevFT
To do this we follow the methodology as laid out in Week 1 of the course.
Full code is available at: https://github.com/jzuniga7/CSML/blob/master/RfPrediction.md
Split data into training, testing and validation

We create the training and test sets from the data provided in the WLE data set. We further split the training set into two (70% and 30% respectively) so that we can cross validate.
# load training set
training <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv")
# load testing set
testing <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv")
# Partition training set to allow cross validation
inTrain <- createDataPartition(training$classe, p=0.7, list=FALSE)
training1 <- training[inTrain,]
training2 <- training[-inTrain,]
In Training, pick features

By looking at the data from:
summary(training)
We can split the features in the data set into four groups.
Group 1: Features that are information about the data set and not good for prediction (e.g. X, user_name)
Group 2: Features that have mostly blank values (eg. kurtosis_roll_belt, skewness_roll_belt, amplitude_yaw_dumbbell, etc.)
Group 3: Features that are mostly NAs (e.g. amplitude_roll_belt, min_roll_arm, amplitude_roll_dumbbell, etc.)
Group 4: Features that have good coverage of values and would be good predictors
Groups 1-3 are not good predictors and we therefore weed them out from the data set, leaving only Group 4 and classe.
predictors <- c("classe", "roll_belt","pitch_belt", "yaw_belt","total_accel_belt", "gyros_belt_x",
"gyros_belt_y", "gyros_belt_z", "accel_belt_x", "accel_belt_y", "accel_belt_z", "magnet_belt_x", "magnet_belt_y", "magnet_belt_z", "roll_arm","pitch_arm", "yaw_arm", "total_accel_arm", "gyros_arm_x", "gyros_arm_y", "gyros_arm_z","accel_arm_x", "accel_arm_y", "accel_arm_z", "magnet_arm_x", "magnet_arm_y", "magnet_arm_z", "roll_dumbbell", "pitch_dumbbell", "yaw_dumbbell", "gyros_dumbbell_x", "gyros_dumbbell_y", "gyros_dumbbell_z","accel_dumbbell_x", "accel_dumbbell_y", "accel_dumbbell_z", "magnet_dumbbell_x", "magnet_dumbbell_y", "magnet_dumbbell_z", "roll_forearm", "pitch_forearm", "yaw_forearm", "total_accel_forearm", "total_accel_dumbbell",
"gyros_forearm_x", "gyros_forearm_y", "gyros_forearm_z", "accel_forearm_x", "accel_forearm_y", "accel_forearm_z", "magnet_forearm_x", "magnet_forearm_y", "magnet_forearm_z")
training1 <- training1[,predictors]
training2 <- training2[,predictors]
In training, pick prediction function

We can now train the model using the random forest method as instructed and then use it to predict the training2 data to verify it is working well.
modFit <- randomForest(factor(classe)~., data=training1, importance=TRUE)
modPredict <- predict(modFit, newdata=training2)
confusionMatrix(modPredict, training2$classe)
Confusion Matrix and Statistics
          Reference
Prediction    A    B    C    D    E
         A 1674    1    0    0    0
         B    0 1132    3    0    0
         C    0    6 1021    2    0
         D    0    0    2  962    2
         E    0    0    0    0 1080
Overall Statistics
                                         
               Accuracy : 0.9973          
                 95% CI : (0.9956, 0.9984)
    No Information Rate : 0.2845          
    P-Value [Acc > NIR] : < 2.2e-16      
                                         
                  Kappa : 0.9966          
 Mcnemar's Test P-Value : NA              
Statistics by Class:
                     Class: A Class: B Class: C Class: D Class: E
Sensitivity            1.0000   0.9939   0.9951   0.9979   0.9982
Specificity            0.9998   0.9994   0.9984   0.9992   1.0000
Pos Pred Value         0.9994   0.9974   0.9922   0.9959   1.0000
Neg Pred Value         1.0000   0.9985   0.9990   0.9996   0.9996
Prevalence             0.2845   0.1935   0.1743   0.1638   0.1839
Detection Rate         0.2845   0.1924   0.1735   0.1635   0.1835
Detection Prevalence   0.2846   0.1929   0.1749   0.1641   0.1835
Balanced Accuracy      0.9999   0.9966   0.9967   0.9986   0.9991
Predicting the error

Based on prediction on the training2 data set, the accuracy was very high with misclassifications for A:1/1674, B:3/1132, C:6/1021, D:2/962, E:0/1080. With an accuracy of 99.73%, the out of sample error is .27%.
Predict the testing data set

With the model verified, we can now use the model to predict 20 cases from the testing data set.
modPredict2 <- predict(modFit, newdata=testing)
> modPredict2
 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20
 B  A  B  A  A  E  D  B  A  A  B  C  B  A  E  E  A  B  B  B
Levels: A B C D E
